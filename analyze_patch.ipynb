{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f753135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ライブラリのインポート\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionInpaintPipeline \n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import datasets, transforms # datasets を追加\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patheffects as pe\n",
    "from collections import Counter\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. パラメータ設定\n",
    "# ==============================================================================\n",
    "STANDARD_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "INPAINT_MODEL_ID = \"runwayml/stable-diffusion-inpainting\"\n",
    "\n",
    "DATA_ROOT = \"./dataset\"#stl10_binaryがあるディレクトリパス\n",
    "\n",
    "NUM_IMAGES_TO_VISUALIZE = 5\n",
    "NUM_TIMESTEP_SAMPLES = 100 \n",
    "NOISE_LEVEL_MIN = 100\n",
    "NOISE_LEVEL_MAX = 900\n",
    "PATCH_GRID_SIZE = 5  #N*Nに分割\n",
    "SEED = 45\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- 乱数シードの固定 ---\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. モデルとデータのロード\n",
    "# ==============================================================================\n",
    "print(f\"Loading STL-10 dataset (test split) from {DATA_ROOT}...\")\n",
    "\n",
    "\n",
    "try:\n",
    "    stl10_dataset_pil = datasets.STL10(root=DATA_ROOT, split='test', download=True)\n",
    "    class_names = stl10_dataset_pil.classes\n",
    "    NUM_CLASSES = len(class_names)\n",
    "    print(f\"  STL-10 dataset loaded. Found {NUM_CLASSES} classes: {class_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"エラー: STL-10データセットのロードに失敗しました。パス '{DATA_ROOT}' を確認してください。: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 関数定義\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 関数: モデルのロード ---\n",
    "def load_model(model_id, pipeline_class):\n",
    "    print(f\"Loading {pipeline_class.__name__}: {model_id}\")\n",
    "    try:\n",
    "        pipe = pipeline_class.from_pretrained(model_id, torch_dtype=torch.float16).to(DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not load fp16 model: {e}. Trying with full precision.\")\n",
    "        pipe = pipeline_class.from_pretrained(model_id).to(DEVICE)\n",
    "    pipe.set_progress_bar_config(disable=True)\n",
    "    print(f\"  Model loaded.\")\n",
    "    return pipe\n",
    "\n",
    "# --- 関数: テキスト埋め込みの事前計算 ---\n",
    "def precompute_embeddings(pipe, class_names):\n",
    "    print(\"Pre-computing text embeddings...\")\n",
    "    tokenizer = pipe.tokenizer\n",
    "    text_encoder = pipe.text_encoder\n",
    "    with torch.no_grad():\n",
    "        uncond_input = tokenizer(\"\", padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "        uncond_embeddings = text_encoder(uncond_input.input_ids.to(DEVICE))[0]\n",
    "        all_class_prompts = [f\"a photo of a {name}\" for name in class_names]\n",
    "        text_inputs = tokenizer(all_class_prompts, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "        all_text_embeddings = text_encoder(text_inputs.input_ids.to(DEVICE))[0]\n",
    "    return torch.cat([uncond_embeddings, all_text_embeddings]).to(DEVICE)\n",
    "\n",
    "# --- 関数: 標準モデルのパッチ分析 (analyze_standard_patch) ---\n",
    "@torch.no_grad()\n",
    "def analyze_standard_patch(pipe, init_latents, noise_timestep_list, text_embeds_batch, true_label):\n",
    "    unet, scheduler = pipe.unet, pipe.scheduler \n",
    "    patch_correct_counts = np.zeros((PATCH_GRID_SIZE, PATCH_GRID_SIZE))\n",
    "    patch_winner_counts = np.zeros((NUM_CLASSES, PATCH_GRID_SIZE, PATCH_GRID_SIZE))\n",
    "    sum_of_pred_variances = np.zeros((PATCH_GRID_SIZE, PATCH_GRID_SIZE))\n",
    "    overall_correct_timesteps = 0\n",
    "    batch_size = text_embeds_batch.shape[0]\n",
    "\n",
    "    for noise, start_timestep in noise_timestep_list:\n",
    "        noisy_latents = scheduler.add_noise(init_latents, noise, torch.tensor([start_timestep], device=DEVICE))\n",
    "        \n",
    "        latent_model_input = noisy_latents.repeat(batch_size, 1, 1, 1)\n",
    "        timestep_input = torch.tensor([start_timestep], device=DEVICE).repeat(batch_size)\n",
    "        pred_noise_batch = unet(latent_model_input, timestep_input, encoder_hidden_states=text_embeds_batch).sample\n",
    "        \n",
    "        error_maps = (pred_noise_batch - noise)**2\n",
    "        spatial_error_maps = error_maps.mean(dim=1) \n",
    "        patch_errors = F.adaptive_avg_pool2d(spatial_error_maps, (PATCH_GRID_SIZE, PATCH_GRID_SIZE)) \n",
    "        \n",
    "        class_pred_noise_batch = pred_noise_batch[1:] \n",
    "        variance_of_preds = torch.var(class_pred_noise_batch, dim=0, unbiased=False) \n",
    "        spatial_variance_map = variance_of_preds.mean(dim=0)\n",
    "        patch_variance = F.adaptive_avg_pool2d(spatial_variance_map.unsqueeze(0), (PATCH_GRID_SIZE, PATCH_GRID_SIZE)).squeeze(0)\n",
    "        sum_of_pred_variances += patch_variance.cpu().numpy()\n",
    "        \n",
    "        class_patch_errors = patch_errors[1:].cpu().numpy()\n",
    "        avg_errors_per_class = class_patch_errors.mean(axis=(1, 2))\n",
    "        if np.argmin(avg_errors_per_class) == true_label:\n",
    "            overall_correct_timesteps += 1\n",
    "        patch_predictions = np.argmin(class_patch_errors, axis=0)\n",
    "        patch_correct_counts += (patch_predictions == true_label)\n",
    "        for i in range(PATCH_GRID_SIZE):\n",
    "            for j in range(PATCH_GRID_SIZE):\n",
    "                patch_winner_counts[patch_predictions[i, j], i, j] += 1\n",
    "\n",
    "    accuracy_map = (patch_correct_counts / NUM_TIMESTEP_SAMPLES) * 100\n",
    "    avg_pred_variance_map = sum_of_pred_variances / NUM_TIMESTEP_SAMPLES\n",
    "    most_frequent_winner_map = np.argmax(patch_winner_counts, axis=0)\n",
    "    overall_accuracy = (overall_correct_timesteps / NUM_TIMESTEP_SAMPLES) * 100\n",
    "    \n",
    "    return accuracy_map, avg_pred_variance_map, most_frequent_winner_map, overall_accuracy\n",
    "\n",
    "# --- 関数: Inpaintingモデルのパッチ分析 (analyze_inpainting_patch) ---\n",
    "@torch.no_grad()\n",
    "def analyze_inpainting_patch(pipe, init_latents_common, noise_timestep_list, text_embeds_batch, true_label, image_pil_resized_for_analysis):\n",
    "    vae, unet, scheduler = pipe.vae, pipe.unet, pipe.scheduler\n",
    "    batch_size = text_embeds_batch.shape[0]\n",
    "    dtype = text_embeds_batch.dtype\n",
    "\n",
    "    accuracy_map = np.zeros((PATCH_GRID_SIZE, PATCH_GRID_SIZE))\n",
    "    avg_pred_variance_map = np.zeros((PATCH_GRID_SIZE, PATCH_GRID_SIZE))\n",
    "    most_frequent_winner_map = np.zeros((PATCH_GRID_SIZE, PATCH_GRID_SIZE), dtype=int)\n",
    "    patch_size_img = 512 // PATCH_GRID_SIZE\n",
    "\n",
    "    image_transform_inner = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "    mask_transform_inner = transforms.ToTensor()\n",
    "    \n",
    "    image_tensor_common = image_transform_inner(image_pil_resized_for_analysis).unsqueeze(0).to(DEVICE, dtype=dtype)\n",
    "    \n",
    "    desc_inner = f\"Analyzing {PATCH_GRID_SIZE**2} patches (Inpainting)\"\n",
    "    for i in tqdm(range(PATCH_GRID_SIZE), desc=desc_inner, leave=False):\n",
    "        for j in range(PATCH_GRID_SIZE):\n",
    "            mask_pil = Image.new(\"L\", (512, 512), 0)\n",
    "            draw = ImageDraw.Draw(mask_pil)\n",
    "            top_left = (j * patch_size_img, i * patch_size_img)\n",
    "            bottom_right = ((j + 1) * patch_size_img, (i + 1) * patch_size_img)\n",
    "            draw.rectangle([top_left, bottom_right], fill=255)\n",
    "            \n",
    "            current_mask_tensor = mask_transform_inner(mask_pil).unsqueeze(0).to(DEVICE, dtype=dtype)\n",
    "            masked_image_tensor = image_tensor_common * (1 - current_mask_tensor) \n",
    "\n",
    "            mask_latents = F.interpolate(current_mask_tensor, size=init_latents_common.shape[-2:])\n",
    "            masked_image_latents = vae.encode(masked_image_tensor).latent_dist.sample() * 0.18215\n",
    "\n",
    "            patch_winners_list = []\n",
    "            patch_pred_variances_list = []\n",
    "\n",
    "            for noise, start_timestep in noise_timestep_list:\n",
    "                \n",
    "                noisy_latents = scheduler.add_noise(init_latents_common, noise, torch.tensor([start_timestep], device=DEVICE))\n",
    "                \n",
    "                latent_model_input_ = torch.cat([noisy_latents] * batch_size)\n",
    "                mask_input_ = torch.cat([mask_latents] * batch_size)\n",
    "                masked_latents_input_ = torch.cat([masked_image_latents] * batch_size)\n",
    "                latent_model_input = torch.cat([latent_model_input_, mask_input_, masked_latents_input_], dim=1)\n",
    "                \n",
    "                timestep_input = torch.tensor([start_timestep], device=DEVICE).repeat(batch_size)\n",
    "                pred_noise_batch = unet(latent_model_input, timestep_input, encoder_hidden_states=text_embeds_batch).sample\n",
    "                class_pred_noise_batch = pred_noise_batch[1:] \n",
    "                \n",
    "                errors = [F.mse_loss(pred * mask_latents, noise * mask_latents).item() for pred in class_pred_noise_batch]\n",
    "                winner_idx = np.argmin(errors)\n",
    "                patch_winners_list.append(winner_idx)\n",
    "                \n",
    "                masked_preds = class_pred_noise_batch * mask_latents\n",
    "                variance_of_preds = torch.var(masked_preds, dim=0, unbiased=False)\n",
    "                spatial_variance_map = variance_of_preds.mean(dim=0)\n",
    "                mask_pixels = mask_latents.squeeze() > 0\n",
    "                if mask_pixels.any():\n",
    "                        patch_variance_value = spatial_variance_map[mask_pixels].mean().item()\n",
    "                else:\n",
    "                       patch_variance_value = 0.0 \n",
    "                patch_pred_variances_list.append(patch_variance_value)\n",
    "\n",
    "            accuracy_map[i, j] = (patch_winners_list.count(true_label) / NUM_TIMESTEP_SAMPLES) * 100\n",
    "            avg_pred_variance_map[i, j] = np.mean(patch_pred_variances_list) if patch_pred_variances_list else 0.0\n",
    "            most_frequent_winner_map[i, j] = Counter(patch_winners_list).most_common(1)[0][0] if patch_winners_list else -1\n",
    "\n",
    "    return accuracy_map, avg_pred_variance_map, most_frequent_winner_map, -1.0 \n",
    "\n",
    "# ==============================================================================\n",
    "# 5. メイン実行ループ\n",
    "# ==============================================================================\n",
    "print(f\"\\nSelecting {NUM_IMAGES_TO_VISUALIZE} random images from STL-10...\")\n",
    "all_indices = list(range(len(stl10_dataset_pil)))\n",
    "selected_indices = random.sample(all_indices, NUM_IMAGES_TO_VISUALIZE)\n",
    "\n",
    "results_standard = {}\n",
    "results_inpaint = {}\n",
    "\n",
    "# --- 5.1 標準モデルでの計算 ---\n",
    "pipe_std = load_model(STANDARD_MODEL_ID, StableDiffusionPipeline)\n",
    "text_embeds_std = precompute_embeddings(pipe_std, class_names)\n",
    "\n",
    "vae_std = pipe_std.vae\n",
    "scheduler_std = pipe_std.scheduler\n",
    "sd_transform_for_latents = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "print(\"\\n--- Running Standard Model Analysis ---\")\n",
    "for image_index in tqdm(selected_indices, desc=\"Standard Model\"):\n",
    "    image_pil_original, true_label = stl10_dataset_pil[image_index]\n",
    "    if image_pil_original.mode != 'RGB':\n",
    "        image_pil_original = image_pil_original.convert('RGB')\n",
    "        \n",
    "    image_pil_resized_std = image_pil_original.resize((512, 512)) # リサイズ\n",
    "    \n",
    "    # 1. 共通のinit_latentsをここで1回だけ生成\n",
    "    image_tensor = sd_transform_for_latents(image_pil_resized_std).unsqueeze(0).to(DEVICE, dtype=text_embeds_std.dtype)\n",
    "    with torch.no_grad():\n",
    "        init_latents = vae_std.encode(image_tensor).latent_dist.sample() * 0.18215\n",
    "        \n",
    "    # 2. 共通のノイズとタイムステップのリストをここで1回だけ生成\n",
    "    noise_timestep_list = []\n",
    "    for _ in range(NUM_TIMESTEP_SAMPLES):\n",
    "        noise = torch.randn_like(init_latents)\n",
    "        start_timestep = random.randint(NOISE_LEVEL_MIN, NOISE_LEVEL_MAX)\n",
    "        noise_timestep_list.append((noise, start_timestep))\n",
    "    \n",
    "    acc_map, var_map, win_map, overall_acc = analyze_standard_patch(\n",
    "        pipe_std, init_latents, noise_timestep_list, text_embeds_std, true_label \n",
    "    )\n",
    "    \n",
    "    results_standard[image_index] = {\n",
    "        'acc': acc_map, 'var': var_map, 'win': win_map, 'overall': overall_acc,\n",
    "        'image_pil': image_pil_resized_std, \n",
    "        'init_latents': init_latents, \n",
    "        'noise_list': noise_timestep_list,\n",
    "        'true_label': true_label # ★true_labelも保存\n",
    "    }\n",
    "\n",
    "del vae_std, scheduler_std\n",
    "del pipe_std, text_embeds_std \n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- 5.2 Inpaintingモデルでの計算 ---\n",
    "pipe_inpaint = load_model(INPAINT_MODEL_ID, StableDiffusionInpaintPipeline)\n",
    "text_embeds_inpaint = precompute_embeddings(pipe_inpaint, class_names)\n",
    "\n",
    "print(\"\\n--- Running Inpainting Model Analysis ---\")\n",
    "for image_index in tqdm(selected_indices, desc=\"Inpainting Model\"):\n",
    "    \n",
    "    if image_index not in results_standard:\n",
    "        print(f\"Skipping inpaint for index {image_index} as standard results are missing.\")\n",
    "        continue\n",
    "        \n",
    "    # 標準モデルで保存したアセットを取得\n",
    "    res_std_item = results_standard[image_index]\n",
    "    true_label = res_std_item['true_label'] # ★保存したtrue_labelを使用\n",
    "    image_pil_resized_inp = res_std_item['image_pil'] \n",
    "    init_latents_common = res_std_item['init_latents'] \n",
    "    noise_list_common = res_std_item['noise_list'] \n",
    "\n",
    "    acc_map, var_map, win_map, _ = analyze_inpainting_patch(\n",
    "        pipe_inpaint, init_latents_common, noise_list_common, text_embeds_inpaint, true_label, image_pil_resized_inp\n",
    "    )\n",
    "    \n",
    "    results_inpaint[image_index] = {\n",
    "        'acc': acc_map, 'var': var_map, 'win': win_map\n",
    "    }\n",
    "del pipe_inpaint, text_embeds_inpaint \n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. 比較プロットの生成\n",
    "# ==============================================================================\n",
    "print(\"\\nGenerating comparison plots...\")\n",
    "\n",
    "plot_text_settings = {\n",
    "    'ha': \"center\", 'va': \"center\", 'fontsize': 12, 'fontweight': 'bold',\n",
    "    'path_effects': [pe.Stroke(linewidth=1.5, foreground='black'), pe.Normal()]\n",
    "}\n",
    "plot_text_settings_small = {\n",
    "    'ha': \"center\", 'va': \"center\", 'fontsize': 10, 'fontweight': 'bold',\n",
    "    'path_effects': [pe.Stroke(linewidth=2, foreground='black'), pe.Normal()]\n",
    "}\n",
    "\n",
    "for idx in selected_indices:\n",
    "    \n",
    "    if idx not in results_standard or idx not in results_inpaint:\n",
    "        print(f\"Skipping plot for index {idx} due to missing results.\")\n",
    "        continue\n",
    "        \n",
    "    res_std = results_standard[idx]\n",
    "    res_inp = results_inpaint[idx]\n",
    "\n",
    "    true_label = res_std['true_label'] \n",
    "    true_label_name = class_names[true_label]\n",
    "    image_pil = res_std['image_pil'] \n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(32, 16)) \n",
    "    fig.suptitle(f\"Comparison Analysis for '{true_label_name}' (Index: {idx}, Standard Overall: {res_std['overall']:.0f}%)\", fontsize=20, y=0.97) \n",
    "    \n",
    "    # --- 共通の分散スケールを「この画像」の標準モデルから決定 ---\n",
    "    std_var_map = res_std['var']\n",
    "    vmin_var_local, vmax_var_local = std_var_map.min(), std_var_map.max()\n",
    "    if vmin_var_local == vmax_var_local:\n",
    "        norm_var_local = mcolors.Normalize(vmin=vmin_var_local - 1e-6, vmax=vmax_var_local + 1e-6)\n",
    "    else:\n",
    "        norm_var_local = mcolors.Normalize(vmin=vmin_var_local, vmax=vmax_var_local)\n",
    "    cmap_var = plt.get_cmap('magma')\n",
    "\n",
    "    # --- 1行目: 標準モデル ---\n",
    "    axes[0, 0].imshow(image_pil); axes[0, 0].set_title(\"Original Image (Standard Model Row)\"); axes[0, 0].axis('off')\n",
    "    \n",
    "    ax = axes[0, 1]; ax.imshow(image_pil)\n",
    "    cmap_acc = plt.get_cmap('viridis'); norm_acc = mcolors.Normalize(vmin=0, vmax=100)\n",
    "    map_data = res_std['acc']\n",
    "    resized_heatmap = Image.fromarray(map_data.astype(np.float32)).resize((512, 512), Image.NEAREST)\n",
    "    im = ax.imshow(np.array(resized_heatmap), cmap=cmap_acc, norm=norm_acc, alpha=0.6)\n",
    "    fig.colorbar(im, ax=ax, label='Patch Accuracy (%)', shrink=0.7)\n",
    "    ax.set_title(\"Std: Patch Accuracy (vs True)\")\n",
    "    for r in range(PATCH_GRID_SIZE):\n",
    "        for c in range(PATCH_GRID_SIZE):\n",
    "            val = map_data[r, c]\n",
    "            text_color = \"k\" if val > 60 else \"w\"\n",
    "            ax.text((c+0.5)*(512/PATCH_GRID_SIZE), (r+0.5)*(512/PATCH_GRID_SIZE), f\"{val:.0f}%\", color=text_color, **plot_text_settings)\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax = axes[0, 2]; ax.imshow(image_pil)\n",
    "    map_data = res_std['var']\n",
    "    resized_heatmap = Image.fromarray(map_data.astype(np.float32)).resize((512, 512), Image.NEAREST)\n",
    "    im = ax.imshow(np.array(resized_heatmap), cmap=cmap_var, norm=norm_var_local, alpha=0.6) \n",
    "    fig.colorbar(im, ax=ax, label='Avg. Pred. Noise Variance', shrink=0.7)\n",
    "    ax.set_title(\"Std: Prediction Variance (Importance)\"); ax.axis('off')\n",
    "\n",
    "    ax = axes[0, 3]; ax.imshow(image_pil)\n",
    "    cmap_winner = plt.get_cmap('tab10', len(class_names)); norm_winner = mcolors.Normalize(vmin=-0.5, vmax=len(class_names)-0.5)\n",
    "    map_data = res_std['win']\n",
    "    resized_heatmap = Image.fromarray(map_data.astype(np.float32)).resize((512, 512), Image.NEAREST)\n",
    "    im = ax.imshow(np.array(resized_heatmap), cmap=cmap_winner, norm=norm_winner, alpha=0.6)\n",
    "    cbar = fig.colorbar(im, ax=ax, label='Most Frequent Winner Class', shrink=0.7)\n",
    "    cbar.set_ticks(np.arange(len(class_names)))\n",
    "    cbar.set_ticklabels(class_names)\n",
    "    ax.set_title(\"Std: Most Frequent Winner\")\n",
    "    for r in range(PATCH_GRID_SIZE):\n",
    "        for c in range(PATCH_GRID_SIZE):\n",
    "            winner_idx = map_data[r, c]; \n",
    "            winner_name = class_names[winner_idx]\n",
    "            text_color = \"lime\" if winner_idx == true_label else \"red\"\n",
    "            ax.text((c+0.5)*(512/PATCH_GRID_SIZE), (r+0.5)*(512/PATCH_GRID_SIZE), winner_name, color=text_color, **plot_text_settings_small)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # --- 2行目: Inpaintingモデル ---\n",
    "    axes[1, 0].imshow(image_pil); axes[1, 0].set_title(\"Original Image (Inpainting Model Row)\"); axes[1, 0].axis('off')\n",
    "    \n",
    "    ax = axes[1, 1]; ax.imshow(image_pil)\n",
    "    map_data = res_inp['acc'] \n",
    "    resized_heatmap = Image.fromarray(map_data.astype(np.float32)).resize((512, 512), Image.NEAREST)\n",
    "    im = ax.imshow(np.array(resized_heatmap), cmap=cmap_acc, norm=norm_acc, alpha=0.6)\n",
    "    fig.colorbar(im, ax=ax, label='Patch Accuracy (%)', shrink=0.7)\n",
    "    ax.set_title(\"Inpaint: Patch Accuracy (In-Mask)\")\n",
    "    for r in range(PATCH_GRID_SIZE):\n",
    "        for c in range(PATCH_GRID_SIZE):\n",
    "            val = map_data[r, c]\n",
    "            text_color = \"k\" if val > 60 else \"w\"\n",
    "            ax.text((c+0.5)*(512/PATCH_GRID_SIZE), (r+0.5)*(512/PATCH_GRID_SIZE), f\"{val:.0f}%\", color=text_color, **plot_text_settings)\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax = axes[1, 2]; ax.imshow(image_pil)\n",
    "    map_data = res_inp['var'] \n",
    "    resized_heatmap = Image.fromarray(map_data.astype(np.float32)).resize((512, 512), Image.NEAREST)\n",
    "    im = ax.imshow(np.array(resized_heatmap), cmap=cmap_var, norm=norm_var_local, alpha=0.6) \n",
    "    fig.colorbar(im, ax=ax, label='Avg. Pred. Noise Variance', shrink=0.7)\n",
    "    ax.set_title(\"Inpaint: Prediction Variance (In-Mask)\"); ax.axis('off')\n",
    "\n",
    "    ax = axes[1, 3]; ax.imshow(image_pil)\n",
    "    map_data = res_inp['win'] \n",
    "    resized_heatmap = Image.fromarray(map_data.astype(np.float32)).resize((512, 512), Image.NEAREST)\n",
    "    im = ax.imshow(np.array(resized_heatmap), cmap=cmap_winner, norm=norm_winner, alpha=0.6)\n",
    "    cbar = fig.colorbar(im, ax=ax, label='Most Frequent Winner Class', shrink=0.7)\n",
    "    cbar.set_ticks(np.arange(len(class_names)))\n",
    "    cbar.set_ticklabels(class_names)\n",
    "    ax.set_title(\"Inpaint: Most Frequent Winner (In-Mask)\")\n",
    "    for r in range(PATCH_GRID_SIZE):\n",
    "        for c in range(PATCH_GRID_SIZE):\n",
    "            winner_idx = map_data[r, c]\n",
    "            if winner_idx == -1:\n",
    "                winner_name = \"ERR\"; text_color = \"gray\"\n",
    "            else:\n",
    "                winner_name = class_names[winner_idx]\n",
    "                text_color = \"lime\" if winner_idx == true_label else \"red\"\n",
    "            ax.text((c+0.5)*(512/PATCH_GRID_SIZE), (r+0.5)*(512/PATCH_GRID_SIZE), winner_name, color=text_color, **plot_text_settings_small)\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n",
    "    plt.show()\n",
    "\n",
    "# --- 7. クリーンアップ ---\n",
    "print(\"\\nCleaning up final resources...\")\n",
    "variables_to_delete = ['results_standard', 'results_inpaint', 'stl10_dataset_pil',\n",
    "                       'selected_indices', 'sd_transform_for_latents']\n",
    "for var_name in variables_to_delete:\n",
    "    if var_name in locals() or var_name in globals():\n",
    "        try: exec(f\"del {var_name}\")\n",
    "        except NameError: pass \n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "print(\"--- Comparison analysis finished. ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
